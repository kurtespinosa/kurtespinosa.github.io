---
layout: page
title: About
subtitle: Kurt Junshean Espinosa
desc: About me.
permalink: /about/
---

<div class="pretty-links">

<div class="lead lead-about">My research interest lies at the intersection of machine learning and natural language processing. I am largely motivated by the challenge of how machines can learn to read and understand textual data.
</div>

{::nomarkdown} 
<figure class="site-profile">
    <img src="{{ site.baseurl }}/assets/img/profile.jpg">
</figure>
{:/}
 All the time we use terms (i.e. words or phrases) to illustrate concepts and communicate our ideas. How can we teach the computers to learn the meaning of these terms? Are there algorithms to learn the different terms of the same concept across domains? I propose that the representation process can be done at the lexical, syntactic, and semantic levels. For example, non-standard English and out-of-vocabulary words in social media will be normalised (e.g., “idk”, “dunno” will be converted to “I do not know”). On the other hand, there are different ways of describing a concept depending on the context. For example, someone who is alcoholic may be described differently: in social media as “boozer”, in clinical records as simply “alcoholic”, scientific literature as “suffering alcoholic dependency”. 

 I would like to investigate different techniques, combining both levels of representation in order to achieve high accuracy in various domains. In this way, textual data from various resources, e.g., scientific literature, clinical records, social media, can be seamlessly integrated to facilitate knowledge discovery.

Here is my [CV]().


---
## Activities

**2016 September: Joined the COLING Twitter NER Shared-Task - won 3rd place!**
- The [task](http://noisy-text.github.io/2016/index.html) consists of identification of named-entities in a tweet and classification of its entity type (e.g. location, person, organisation, etc). [[paper](http://www.aclweb.org/anthology/W/W16/W16-39.pdf#page=165)]

</div>