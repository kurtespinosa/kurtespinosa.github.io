---
title: What was the COLING for me?
desc: My experience in attending the COLING conference in Osaka, Japan.
---

It was a terribly good experience! Understanding sentences like that remains one of the challenges in Natural Language Processing (NLP) in general based on my impression after attending the Computational Linguistics (COLING) conference held at Osaka, Japan from Dec 10-16, 2016. In this post, I aim to summarise my learning experiences in COLING.

My proposed topic (semantic similarity) is a core problem in many high-level semantic applications such as question answering, summarisation, entailment, translation, etc. In fact, my observation is confirmed by [Omer Levy](https://levyomer.wordpress.com/bio/) (University of Washington and Bar-Ilan University, Israel) whom I also met during the conference. 

I also had a short discussion with  [Omer Levy](https://levyomer.wordpress.com/bio/), [Pontus Stenetorp](http://pontus.stenetorp.se/) (UCL) and [Dimitrios Kartsaklis](http://www.eecs.qmul.ac.uk/people/view/45712/dimitrios-kartsaklis) (Queen Mary University of London) among others about sentence compositionally on what they think about it. They know the problem well and just simply responded what would be an alternative. I proposed that it should stop at semantic units (which could be at the sentence level) and from there do the alignments. I shared this intuition with Omer and Dimitrios at least in my impression. 

I also discovered interesting and intuitive deep learning architectures for capturing language intuitions. I'm listing down below the especially interesting ones for me:

- Using attention to dynamically decide between character or word embeddings on the input representation by [Marek Rei](http://www.marekrei.com/) (University of Cambridge). [Attending to Characters in Neural Sequence Labeling Models](https://arxiv.org/abs/1611.04361)

- Using orthographic sequence representation to capture noise in twitter by [Nut Limsopatham](http://www.dcs.gla.ac.uk/~nutli/) (University of Cambridge). [Bidirectional LSTM for Named Entity Recognition in Twitter Messages](http://noisy-text.github.io/2016/pdf/WNUT20.pdf)

- Lexical Decomposition and Composition by [Zhiguo Wang](http://researcher.watson.ibm.com/researcher/view.php?person=us-zhigwang) (IBM New York). [Sentence Similarity Learning by Lexical Decomposition and Composition](https://arxiv.org/abs/1602.07019)

- Re-reading of sentence according to memory of another sentence for better understanding by [Lei Sha](http://shalei120.github.io/) et al (Peking University). [Reading and Thinking: Re-read LSTM Unit for Textual Entailment Recognition](http://www.aclweb.org/anthology/C/C16/C16-1270.pdf)

- Alignment with implicit distortion and fertility using attention by Shi Feng et al (University of Maryland College Park). [Improving Attention Modeling with Implicit Distortion and Fertility for Machine Translation](http://www.aclweb.org/anthology/C/C16/C16-1290.pdf)

- Applying attention to tree structures by [Kai Zhao](http://kaizhao.me/) (Oregon State University). [Textual Entailment with Structured Attentions and Composition](http://kaizhao.me/files/structured-attention.pdf)

I also found interesting studies that try to capture linguistic notions from humans and behaviour.

- Omer Levy on using subtree entailment to extract sentence intersection. [Modeling Extractive Sentence Intersection via Subtree Entailment](https://levyomer.files.wordpress.com/2016/12/modeling-extractive-sentence-intersection-via-subtree-entailment-coling-2016.pdf)

- [Barbara Plank](http://www.let.rug.nl/~bplank/) (University of Groningen, Netherlands) on modelling syntactic parsing via keystroke dynamics. [Keystroke dynamics as signal for shallow syntactic parsing](https://arxiv.org/abs/1610.03321)

- Lieke Gelderloos (Tilburg University, Netherlands) on levels of representations of RNN using images and phonemes. [From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning](https://arxiv.org/abs/1610.03342)

- [Marie Candito](http://www.linguist.univ-paris-diderot.fr/~mcandito/) (Universit√© Paris Diderot) used syntactic rules for semantic parsing. [Deeper syntax for better semantic parsing](https://hal.archives-ouvertes.fr/hal-01391678/document). If it is possible to model this using deep neural networks, then it would be great! 

Related to this, I was thinking of doing an experiment that simulates alignment. It should simulate aspects of iSTS such as alignment type. Further, timestamp should be noted to indicate the order of importance when doing alignment. 
In addition, I wonder also if the work of Barbara could be done for speech for the same task of shallow syntactic parsing.

Further, there were some studies focusing more on machine learning.

- [Marten Postma](https://martenpostma.com/) (Vrije Universiteit Amsterdam, Netherlands) More data does not always result to better performance since most frequently occurring word-sense gets selected. The conclusion is that acquisition of training data must be guided by the task at hand. [More is not always better: balancing sense distributions for all-words Word Sense Disambiguation](http://www.aclweb.org/anthology/C/C16/C16-1330.pdf).

- [Philip Schulz](http://www.uva.nl/over-de-uva/organisatie/medewerkers/content/s/c/p.schulz/p.schulz.html) (University of Amsterdam) on word alignment using HMM and language model. [Fast Collocation-Based Bayesian HMM Word Alignment](http://wilkeraziz.github.io/papers/coling2016.pdf)

And some datasets.

- [Leon Derczynski](http://www.derczynski.com/sheffield/) (University of Sheffield, UK). Twitter corpus that is more representative spatially and temporally. [Broad Twitter Corpus: A Diverse Named Entity Recognition Resource](http://www.derczynski.com/sheffield/papers/btc.pdf). The dataset is free and spans 5 years beginning 2009.

- Had a short chat with [Thomas Francois](http://cental.fltr.ucl.ac.be/team/tfrancois/) (UCL, Belgium) on the possibility of teaching machines like teaching kids to read which is to start from simple sentences and he referred to me a dataset: weekly reader and BBC bitesize. This would be an interesting research.

I also met some fantastic people from industry like the CEO and Co-founder of Chata.ai (based in Calgary, Canada), [Kelly Cherniwchan](https://www.linkedin.com/in/kelly-cherniwchan-2969426/). Imagine a question-answering system that interacts with your database.

Here is the [COLING 2016 proceedings](http://coling2016.anlp.jp/doc/main.pdf).

Lastly, the list of upcoming conferences was announced at the closing event as follows:

EACL 2017. April 3-7, Spain. Deadline passed.

ACL 2017. July 30-Aug 4, Vancouver. Deadline: Feb 6, 2017.

EMNLP 2017. Sep 9-11, Copenhagen. Deadline: April 14, 2017.

COLING 2018. Sta Fe, USA. TBA.

Attending conferences indeed is very expensive but I think it is the most efficient way to get new ideas and to know the state-of-the-art in my field through the face-to-face interaction with the experts themselves in the field.




