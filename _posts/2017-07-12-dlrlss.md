---
title: Frontiers of Deep Learning and Reinforcement Learning 
desc: Some things that I learned during the Deep Learning and Reinforcement Learning Summer School at MILA, Montreal
---

First of all, this post would not claim to exhaustively narrate everything discussed during the summer school. There are [slides](https://mila.umontreal.ca/en/cours/deep-learning-summer-school-2017/schedule/) and [video lectures](http://videolectures.net/site/search/?q=deep+learning+summer+school) for that and I highly recommend that you go thru them. Rather, in this post I will just mention ideas that are either: 1) new to me, 2) old but gain new perspective and deeper understanding 3) plain interesting. Some of these are mentioned in the slides or during the talk.

To give everyone an overview, the talks started with a review of Machine Learning by Dolna Precup (McGill University). She mentioned about parametric vs non-parametric models, bias-variance trade-off, regularisation, etc. 

The next part was an introduction to Neural Networks by Hugo Rochelle (Google Brain). He talked about the basics of neural network learning by backpropagation, etc. He also mentioned all the hyperparameters that need to be tuned especially during optimisation. At the end of his talk, he mentioned about one-shot or zero-shot learning and the need to design new architectures based on intuition. 

Yoshua Bengio (MILA, Uni of Montreal) gave the next talk on Recurrent Neural Networks. He mentioned about sequence-to-sequence network as a version of autoencoder-decoder. He also emphasised that the multiplicative interactions in the network increases expressive power. Regarding the vanishing gradient that on the one hand, if the eigen values are < 1, the gradient approaches zero, thus the vanishing gradient; on the other hand, if the eigen values are > 1, the gradient approaches infinity and thus the exploding gradient problem. He said that in GRUs gradients are copied instead of updated. Furthermore, he said that the main gate in the LSTM network is the forget gate and how the gates work are not yet fully understood. He also mentioned about the use of attention to make the network focus on important information in the network. It is to be noted that he and his group pioneered both RNNs and attention mechanism. Lastly, he said that backpropagation through time (BPTT), which is what happens in the RNNs, does not seem biologically plausible because does not backpropagate the errors to the past just to learn something. During the summer school, Yoshua Bengio was awarded the Order of Canada for his contributions in this field. 

The next talk was about Probabilistic Numerics by Mike Osborne (Oxford). This area sounds new to me but one thing that intrigues me was what he said that "integration beats optimisation". He gave us the [link](http://probabilistic-numerics.org/) about this subject for those interested to go into this field. 

The next talk was about Generative Models by Ian Goodfellow (Google Brain). He began by describing the taxonomy of generative models then explained how Generative Adversarial Networks (GANs) work. What interested me is the idea that GANs can be used to generate models for simulated training data. 

The next talk was given my Mike Osborne (Oxford) about the Future of Work. He presented statistics on which profession will likely be affected by the coming of intelligent machine age but he also emphasised that though this may happen, history shows that new jobs also come out and that it is up to the policy-makers to manage the transition well. He also said that while we have advanced dramatically in AI, having a cleaning assistant robot in the house would not be feasible even in 20-30 years for that seemingly simple job.

The next talk was about Convolutional Neural Networks (CNN) by Richard Zemel (Uni of Toronto). He talked about the advances in CNN but one particular thing that was interesting to me was about highway networks which could be used to dynamically determine when data is passed thru or transferred. 

Raquel Urtasun (Head, Uber Toronto; Prof, Uni of Toronto) gave the next talk about Deep Structured Models. She emphasised that the outputs for these models are statistically dependent. Having a structure in the output are usually Markov Random Fields (MRFs) but as a post-processing step. The complex dependencies wherein multiple variables are predicted fits with the idea of using graphical models and thus the natural connection to deep learning. 

To all the organisers of the summer school, I could not thank you more. I felt I was so lucky to be among the few admitted to attend it. The summer school has opened to me the vast horizon of research frontiers in this field.

