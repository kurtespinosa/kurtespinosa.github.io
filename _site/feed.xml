<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <title></title>
    <description></description>
    <link>http://kurtespinosa.com</link>
    <atom:link href="http://kurtespinosa.comhttp://kurtespinosa.com/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>What was the COLING for me?</title>
        <description>&lt;p&gt;It was a terribly good experience! Understanding sentences like that remains one of the challenges in Natural Language Processing (NLP) in general based on my impression after attending the Computational Linguistics (COLING) conference held at Osaka, Japan from Dec 10-16, 2016. In this post, I aim to summarise my learning experiences in COLING.&lt;/p&gt;

&lt;p&gt;1.My proposed topic (semantic similarity) is a core problem in many high-level semantic applications such as question answering, summarisation, entailment, translation, etc. In fact, my observation is confirmed by &lt;a href=&quot;https://levyomer.wordpress.com/bio/&quot;&gt;Omer Levy&lt;/a&gt; (University of Washington and Bar-Ilan University, Israel) whom I also met during the conference.&lt;/p&gt;

&lt;p&gt;2.I also had a short discussion with  &lt;a href=&quot;https://levyomer.wordpress.com/bio/&quot;&gt;Omer Levy&lt;/a&gt;, &lt;a href=&quot;http://pontus.stenetorp.se/&quot;&gt;Pontus Stenetorp&lt;/a&gt; (UCL) and &lt;a href=&quot;http://www.eecs.qmul.ac.uk/people/view/45712/dimitrios-kartsaklis&quot;&gt;Dimitrios Kartsaklis&lt;/a&gt; (Queen Mary University of London) among others about sentence compositionally on what they think about it. They know the problem well and just simply responded what would be an alternative. I proposed that it should stop at semantic units (which could be at the sentence level) and from there do the alignments. I shared this intuition with Omer and Dimitrios at least in my impression.&lt;/p&gt;

&lt;p&gt;3.I also discovered interesting and intuitive deep learning architectures for capturing language intuitions. I’m listing down below the especially interesting ones for me:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- Using attention to dynamically decide between character or word embeddings on the input representation by [Marek Rei](http://www.marekrei.com/) (University of Cambridge). [Attending to Characters in Neural Sequence Labeling Models](https://arxiv.org/abs/1611.04361)
- Using orthographic sequence representation to capture noise in twitter by [Nut Limsopatham](http://www.dcs.gla.ac.uk/~nutli/) (University of Cambridge). [Bidirectional LSTM for Named Entity Recognition in Twitter Messages](http://noisy-text.github.io/2016/pdf/WNUT20.pdf)
- Lexical Decomposition and Composition by [Zhiguo Wang](http://researcher.watson.ibm.com/researcher/view.php?person=us-zhigwang) (IBM New York). [Sentence Similarity Learning by Lexical Decomposition and Composition](https://arxiv.org/abs/1602.07019)
- Re-reading of sentence according to memory of another sentence for better understanding by [Lei Sha](http://shalei120.github.io/) et al (Peking University). [Reading and Thinking: Re-read LSTM Unit for Textual Entailment Recognition](http://www.aclweb.org/anthology/C/C16/C16-1270.pdf)
- Alignment with implicit distortion and fertility using attention by Shi Feng et al (University of Maryland College Park). [Improving Attention Modeling with Implicit Distortion and Fertility for Machine Translation](http://www.aclweb.org/anthology/C/C16/C16-1290.pdf)
- Applying attention to tree structures by [Kai Zhao](http://kaizhao.me/) (Oregon State University). [Textual Entailment with Structured Attentions and Composition](http://kaizhao.me/files/structured-attention.pdf)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;4.I also found interesting studies that try to capture linguistic notions from humans and behaviour&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- Omer Levy on using subtree entailment to extract sentence intersection. [Modeling Extractive Sentence Intersection via Subtree Entailment](https://levyomer.files.wordpress.com/2016/12/modeling-extractive-sentence-intersection-via-subtree-entailment-coling-2016.pdf)
- [Barbara Plank](http://www.let.rug.nl/~bplank/) (University of Groningen, Netherlands) on modelling syntactic parsing via keystroke dynamics. [Keystroke dynamics as signal for shallow syntactic parsing](https://arxiv.org/abs/1610.03321)
- Lieke Gelderloos (Tilburg University, Netherlands) on levels of representations of RNN using images and phonemes. [From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning](https://arxiv.org/abs/1610.03342)
- [Marie Candito](http://www.linguist.univ-paris-diderot.fr/~mcandito/) (Université Paris Diderot) used syntactic rules for semantic parsing. [Deeper syntax for better semantic parsing](https://hal.archives-ouvertes.fr/hal-01391678/document). If it is possible to model this using deep neural networks, then it would be great! 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Related to this, I was thinking of doing an experiment that simulates alignment. It should simulate aspects of iSTS such as alignment type. Further, timestamp should be noted to indicate the order of importance when doing alignment. 
In addition, I wonder also if the work of Barbara could be done for speech for the same task of shallow syntactic parsing.&lt;/p&gt;

&lt;p&gt;5.Further, there were some studies focusing more on machine learning.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- [Marten Postma](https://martenpostma.com/) (Vrije Universiteit Amsterdam, Netherlands) More data does not always result to better performance since most frequently occurring word-sense gets selected. The conclusion is that acquisition of training data must be guided by the task at hand. [More is not always better: balancing sense distributions for all-words Word Sense Disambiguation](http://www.aclweb.org/anthology/C/C16/C16-1330.pdf).
- [Philip Schulz](http://www.uva.nl/over-de-uva/organisatie/medewerkers/content/s/c/p.schulz/p.schulz.html) (University of Amsterdam) on word alignment using HMM and language model. [Fast Collocation-Based Bayesian HMM Word Alignment](http://wilkeraziz.github.io/papers/coling2016.pdf)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;6.Datasets&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- [Leon Derczynski](http://www.derczynski.com/sheffield/) (University of Sheffield, UK). Twitter corpus that is more representative spatially and temporally. [Broad Twitter Corpus: A Diverse Named Entity Recognition Resource](http://www.derczynski.com/sheffield/papers/btc.pdf). The dataset is free and spans 5 years beginning 2009.
- Had a short chat with [Thomas Francois](http://cental.fltr.ucl.ac.be/team/tfrancois/) (UCL, Belgium) on the possibility of teaching machines like teaching kids to read which is to start from simple sentences and he referred to me a dataset: weekly reader and BBC bitesize. This would be an interesting research.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;I also met some fantastic people from industry like the CEO and Co-founder of Chata.ai (based in Calgary, Canada), &lt;a href=&quot;https://www.linkedin.com/in/kelly-cherniwchan-2969426/&quot;&gt;Kelly Cherniwchan&lt;/a&gt;. Imagine a question-answering system that interacts with your database.&lt;/p&gt;

&lt;p&gt;Here is the &lt;a href=&quot;http://coling2016.anlp.jp/doc/main.pdf&quot;&gt;COLING 2016 proceedings&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Lastly, the list of upcoming conferences was announced at the closing event as follows:&lt;/p&gt;

&lt;p&gt;EACL 2017. April 3-7, Spain. Deadline passed.&lt;/p&gt;

&lt;p&gt;ACL 2017. July 30-Aug 4, Vancouver. Deadline: Feb 6, 2017.&lt;/p&gt;

&lt;p&gt;EMNLP 2017. Sep 9-11, Copenhagen. Deadline: April 14, 2017.&lt;/p&gt;

&lt;p&gt;COLING 2018. Sta Fe, USA. TBA.&lt;/p&gt;

&lt;p&gt;Attending conferences indeed is very expensive but I think it is the most efficient way to get new ideas and to know the state-of-the-art in my field through the face-to-face interaction with the experts themselves in the field.&lt;/p&gt;

</description>
        <pubDate>Sat, 17 Dec 2016 00:00:00 +0000</pubDate>
        <link>http://kurtespinosa.com/2016/coling/</link>
        <guid isPermaLink="true">http://kurtespinosa.com/2016/coling/</guid>
      </item>
    
      <item>
        <title>K-fold Cross Validation in Neural Networks</title>
        <description>&lt;p&gt;What I want to explain here is how to use K-fold cross validation in neural networks. We know that in neural networks, we use the back propagation algorithm to adjust the weights or parameters of the neural network as many times until the error (i.e. the difference of the predicted output and expected output values) are small enough for us. I will call these set of parameters/weights in the neural network as simply model parameters.&lt;/p&gt;

&lt;p&gt;Now, we have heard as well that K-fold cross validation can help determine the best model parameters, that is the set of model parameters that can generalise(i.e. perform well) on unseen data(e.g. held-out test set). So the question is: how can we use K-fold cross validation to do that?&lt;/p&gt;

&lt;p&gt;For example, you were given tweets and the task is named entity recognition (NER). Each tweet has been tokenized and each token labeled with a particular entity type (e.g. location, person, organization, etc). So how do you proceed?&lt;/p&gt;

&lt;p&gt;Using neural network, here’s how you should proceed:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Split the data into k-folds and each fold has train, dev, and test set.&lt;/li&gt;
  &lt;li&gt;For each fold, within an epoch loop, train the neural network parameters with the training set and decide whether to continue training using the dev set. Finally at the end of every epoch, you can test the model performance by using the test set.&lt;/li&gt;
  &lt;li&gt;Compute the final f-score using method 3 in this &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=DA673A6EA027C2E8B1831FD9B9A24A0C?doi=10.1.1.186.8880&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;paper&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Retrain the whole dataset using the neural network param with the best f-score.&lt;/li&gt;
  &lt;li&gt;That’s it! You have the model which can generalise to unseen examples.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://cs229.stanford.edu/notes/cs229-notes5.pdf&quot;&gt;http://cs229.stanford.edu/notes/cs229-notes5.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=DA673A6EA027C2E8B1831FD9B9A24A0C?doi=10.1.1.186.8880&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=DA673A6EA027C2E8B1831FD9B9A24A0C?doi=10.1.1.186.8880&amp;amp;rep=rep1&amp;amp;type=pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jamesmccaffrey.wordpress.com/2013/10/25/k-fold-cross-validation-for-neural-networks/&quot;&gt;https://jamesmccaffrey.wordpress.com/2013/10/25/k-fold-cross-validation-for-neural-networks/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://artint.info/html/ArtInt_189.html&quot;&gt;http://artint.info/html/ArtInt_189.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.quora.com/Why-do-we-have-separation-of-data-into-training-held-out-and-test-data&quot;&gt;https://www.quora.com/Why-do-we-have-separation-of-data-into-training-held-out-and-test-data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/2976452/whats-is-the-difference-between-train-validation-and-test-set-in-neural-networ&quot;&gt;https://stackoverflow.com/questions/2976452/whats-is-the-difference-between-train-validation-and-test-set-in-neural-networ&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sat, 20 Aug 2016 00:00:00 +0100</pubDate>
        <link>http://kurtespinosa.com/2016/kfold/</link>
        <guid isPermaLink="true">http://kurtespinosa.com/2016/kfold/</guid>
      </item>
    
  </channel>
</rss>
